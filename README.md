# AI Code Generation and Testing

This repository includes generated Python code and corresponding unit tests for 30 different coding prompts selected from the **HumanEval** dataset.

The code and tests have been generated by:
- **OpenAI GPT-4 Turbo**
- **Anthropic Claude 3.7 Sonnet**

---

## Project Structure

Each prompt has two corresponding folders:

```
Prompt-<number>/
  ├── Antrophic-Claude/
  │     ├── code.py
  │     └── tests.py
  └── OpenAI-GPT/
        ├── code.py
        └── tests.py
```

- `code.py` — The Python solution generated for the prompt.
- `tests.py` — Unit tests for the corresponding solution.

Each folder includes generated code and tests based on the same prompt utilizing different models to be able to compare models directly.

---

## Technologies Used

- **Python**
- **unittest** — Python's built-in unit testing framework used for writing all test cases.
- **AI Models Used**:
  - [OpenAI GPT-4 Turbo](https://platform.openai.com/)
  - [Anthropic Claude 3.7 Sonnet](https://www.anthropic.com)

---

## Example

Example structure for a prompt:

```python
# code.py
def move_one_ball(arr):
    ...
```

```python
# tests.py
import unittest
from code import move_one_ball

class TestMoveOneBall(unittest.TestCase):
    def test_empty_array(self):
        self.assertTrue(move_one_ball([]))
    
    def test_already_sorted(self):
        self.assertTrue(move_one_ball([1, 2, 3, 4, 5]))
        
    ...
    
if __name__ == '__main__':
    unittest.main()
```

---

## How to Run the Tests

1. Navigate into any model-specific prompt folder (e.g., `Prompt-1/OpenAI-GPT/`).
2. Run the test file:
   
```bash
python tests.py
```

You will see the output from `unittest`, displaying passed/failed tests.

---

## Goals of This Project

- Analyze the **code correctness** between GPT-4 Turbo and Claude 3.5 Sonnet.
- Compare **unit test quality** (completeness, edge case coverage etc.).
---

## License

This repository is licensed under the **MIT License**.

---

## Acknowledgements

- OpenAI for GPT-4 Turbo
- Anthropic for Claude models
- [HumanEval](https://github.com/openai/human-eval) dataset
