# AI Code Generation and Testing

This repository includes generated Python code and corresponding unit tests for 30 different coding prompts selected from the **HumanEval** dataset as well as the integrated tests, codes and coverage analysis scripts.

The code and tests have been generated by:
- **OpenAI GPT-4 Turbo**
- **Anthropic Claude 3.7 Sonnet**

---

## Project Structure

Each prompt has two corresponding folders:

```
Prompt-<number>/
  ├── Antrophic-Claude/
  │     ├── code.py
  │     └── tests.py
  └── OpenAI-GPT/
        ├── code.py
        └── tests.py
```

- `code.py` — The Python solution generated for the prompt.
- `tests.py` — Unit tests for the corresponding solution.

Each folder includes generated code and tests based on the same prompt utilizing different models to be able to compare models directly.

- `Before Update` folder includes the first part codes and tests of this project. It includes the GPT and Cluade generated codes and tests without any modification on prompts or tests.
- `After Update` folder includes the second part codes for the project. It includes the updated test files (updated tests are same in the GPT and Claude folders for this section), and updated codes generated from modified prompts both for GPT and Claude seperately.
- `Non Updated Tests - Updated Codes` folder includes the updated codes for both GPT and Claude however it includes the previous part's tests since we wanted to test the coverage of the first part's tests on modified codes.
- `Integration Tests` include both `combined_modified_code` and `combined_unmodified_code`, modified one is the code that LLMs integrated from three classes but using modified codes. Unmodified is the code that LLMs integrated using unmodified codes from the first part.
- You can find `handwritten_tests` and `tests_by_llm` in the same folder and you can run these tests in the same method we provided below.
---
- Note: Remember to change the import when testing combined_modified_code or combined_unmodified_code in the test files in `Integration Tests` folder to which code you are testing at that moment.

---

## Technologies Used

- **Python**
- **unittest** — Python's built-in unit testing framework used for writing all test cases.
- **AI Models Used**:
  - [OpenAI GPT-4 Turbo](https://platform.openai.com/)
  - [Anthropic Claude 3.7 Sonnet](https://www.anthropic.com)

---

## Example

Example structure for a prompt:

```python
# code.py
def move_one_ball(arr):
    ...
```

```python
# tests.py
import unittest
from code import move_one_ball

class TestMoveOneBall(unittest.TestCase):
    def test_empty_array(self):
        self.assertTrue(move_one_ball([]))
    
    def test_already_sorted(self):
        self.assertTrue(move_one_ball([1, 2, 3, 4, 5]))
        
    ...
    
if __name__ == '__main__':
    unittest.main()
```

---

## How to Run the Tests

1. Navigate into any model-specific prompt folder (e.g., `Prompt-1/OpenAI-GPT/`).
2. Run the test file:
   
```bash
python tests.py
```

You will see the output from `unittest`, displaying passed/failed tests.

---

## Generate Coverages

1. You can generate coverage results for all prompts using the scripts:
- `generate_coverage_after_modifications`,
- `generate_coverage_before_modifications`,
- `generate_coverage_old_tests_new_codes`,
- `generate_integration_coverage`
   
```bash
Remember you need to tweak the scripts a little in order to run the code on your desired tests and codes.
(e.g. You may need to change the Prompt folder path or range in this script.)
```

You will see the output from `unittest`, displaying passed/failed tests.

---

## Goals of This Project

- Analyze the **code correctness** between GPT-4 Turbo and Claude 3.7 Sonnet.
- Compare **unit test quality** (completeness, edge case coverage etc.).
- Analyzing the integration test capability of LLMs and comparing the handwritten and LLM generated tests.
---

## License

This repository is licensed under the **MIT License**.

---

## Acknowledgements

- OpenAI for GPT-4 Turbo
- Anthropic for Claude models
- [HumanEval](https://github.com/openai/human-eval) dataset
